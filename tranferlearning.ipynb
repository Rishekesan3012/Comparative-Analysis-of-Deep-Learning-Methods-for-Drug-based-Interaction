{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5196,"status":"ok","timestamp":1686071631627,"user":{"displayName":"Dumbo 2930","userId":"05345327480332063689"},"user_tz":-330},"id":"oDrBYhcKF-DC"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import csv\n","import random\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import optimizers\n","\n","# Load data\n","data = []  # list of lists of the form [smiles, sequence, pKd]\n","\n","with open('/content/drive/MyDrive/dta_df.csv') as csvfile:\n","    reader = csv.reader(csvfile)\n","    next(reader)  # skip header\n","    for row in reader:\n","        triplet = []\n","        triplet.append(row[0])\n","        triplet.append(row[1])\n","        triplet.append(float(row[2]))\n","        data.append(triplet)\n","\n","random.shuffle(data)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":17287,"status":"ok","timestamp":1686071648912,"user":{"displayName":"Dumbo 2930","userId":"05345327480332063689"},"user_tz":-330},"id":"C4DzyvmlG0Oe"},"outputs":[],"source":["\n","# Separate data into inputs (SMILES and proteins) and labels\n","smiles = [triplet[0] for triplet in data]\n","proteins = [triplet[1] for triplet in data]\n","labels = [triplet[2] for triplet in data]\n","\n","# Split data into train and test sets\n","split = int(0.9 * len(smiles))\n","train_smiles = smiles[:split]\n","test_smiles = smiles[split:]\n","train_proteins = proteins[:split]\n","test_proteins = proteins[split:]\n","train_labels = labels[:split]\n","test_labels = labels[split:]\n","\n","smiles_tokenizer = Tokenizer(char_level=True)\n","smiles_tokenizer.fit_on_texts(train_smiles)\n","train_smiles_seq = smiles_tokenizer.texts_to_sequences(train_smiles)\n","test_smiles_seq = smiles_tokenizer.texts_to_sequences(test_smiles)\n","\n","max_smiles_len = max(len(seq) for seq in train_smiles_seq)\n","train_smiles_seq = pad_sequences(train_smiles_seq, maxlen=max_smiles_len)\n","test_smiles_seq = pad_sequences(test_smiles_seq, maxlen=max_smiles_len)\n","\n","# Tokenize and pad protein sequences\n","protein_tokenizer = Tokenizer(char_level=True)\n","protein_tokenizer.fit_on_texts(train_proteins)\n","train_protein_seq = protein_tokenizer.texts_to_sequences(train_proteins)\n","test_protein_seq = protein_tokenizer.texts_to_sequences(test_proteins)\n","\n","max_protein_len = max(len(seq) for seq in train_protein_seq)\n","train_protein_seq = pad_sequences(train_protein_seq, maxlen=max_protein_len)\n","test_protein_seq = pad_sequences(test_protein_seq, maxlen=max_protein_len)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686071648913,"user":{"displayName":"Dumbo 2930","userId":"05345327480332063689"},"user_tz":-330},"id":"H1-MRPGNG3uE"},"outputs":[],"source":["# Convert the input data to numpy arrays\n","train_smiles_seq = np.array(train_smiles_seq)\n","train_protein_seq = np.array(train_protein_seq)\n","train_labels = np.array(train_labels)\n","\n","test_smiles_seq = np.array(test_smiles_seq)\n","test_protein_seq = np.array(test_protein_seq)\n","test_labels = np.array(test_labels)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mtFtgZmAHo52","executionInfo":{"status":"ok","timestamp":1686074255762,"user_tz":-330,"elapsed":2606851,"user":{"displayName":"Dumbo 2930","userId":"05345327480332063689"}},"outputId":"3443bef3-c740-4b77-fd60-60a1e6249382"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","669/669 [==============================] - 1284s 2s/step - loss: 1.2804\n","Epoch 2/2\n","669/669 [==============================] - 1278s 2s/step - loss: 0.6428\n","75/75 [==============================] - 26s 337ms/step - loss: 0.5306\n","Test Loss: 0.530568540096283\n"]}],"source":["embedding_dim = 32\n","hidden_units = 64\n","\n","smiles_input = tf.keras.Input(shape=(max_smiles_len,))\n","protein_input = tf.keras.Input(shape=(max_protein_len,))\n","\n","smiles_embedding = tf.keras.layers.Embedding(len(smiles_tokenizer.word_index) + 1, embedding_dim)(smiles_input)\n","smiles_lstm = tf.keras.layers.LSTM(hidden_units)(smiles_embedding)\n","\n","protein_embedding = tf.keras.layers.Embedding(len(protein_tokenizer.word_index) + 1, embedding_dim)(protein_input)\n","protein_lstm = tf.keras.layers.LSTM(hidden_units)(protein_embedding)\n","\n","concatenated = tf.keras.layers.concatenate([smiles_lstm, protein_lstm])\n","dense1 = tf.keras.layers.Dense(hidden_units, activation='relu')(concatenated)\n","output = tf.keras.layers.Dense(1, activation='linear')(dense1)\n","\n","model = tf.keras.Model(inputs=[smiles_input, protein_input], outputs=output)\n","\n","# Compile and train the model\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.fit([train_smiles_seq, train_protein_seq], train_labels, epochs=2, batch_size=32)\n","\n","# Evaluate the model on test data\n","test_loss = model.evaluate([test_smiles_seq, test_protein_seq], test_labels)\n","print(\"Test Loss:\", test_loss)\n"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1a4So9htgV0YYO9SHUHjZ4HiFgob0AJ3P","authorship_tag":"ABX9TyNpvKTIxL3bdu8weOYDhKOK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}